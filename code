import numpy as np
import pandas as pd
from dateutil import parser
import matplotlib.pyplot as plt
import pandas as pd
from itertools import combinations
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler

def removeAbsoluteLinear(df):
    duplicate_columns = []
    for col1 in df.columns:
        for col2 in df.columns:
            if col1 != col2 and df[col1].nunique() == df[col2].nunique():
                cond1 = (df.groupby(col1)[col2].nunique() == 1).all()
                cond2 = (df.groupby(col2)[col1].nunique() == 1).all()
                if cond1 and cond2 and col1 not in duplicate_columns and col2 not in duplicate_columns:
                    duplicate_columns.append(col2)

    # Remove duplicate columns based on linearity condition
    unique_columns = df.columns.difference(duplicate_columns)

    # New DataFrame with non-duplicate columns
    df_unique = df[unique_columns]
    return df_unique

def getDateFeatures(data):
    datetime_format = r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{3}Z(\[UTC\])?$|^(\d{2}-\d{2}-\d{4}\s\d{1,2}:\d{2})$|^\d{1,2}/\d{1,2}/\d{4}\s\d{1,2}:\d{2}$'
    
    matching_columns = data.apply(lambda col: col.astype(str).str.match(datetime_format).any())
    datetime_column = matching_columns[matching_columns].index.tolist()
    return datetime_column

def getOrdinalFeatures():
    return ['reportableData.transactionType', 'reportableData.lifecycleEvent', 'reportableData.action']

def fillMissingDiscreteNulls(data):
    disc_cols = getDiscreteFeatures(data)
    data[disc_cols] = data[disc_cols].fillna('Novalue')
    return data

def getNumericColumns(data): 
    numeric_columns = data.select_dtypes(include=['int16', 'float', 'int32', 'int64']).columns.tolist()   
    bin_cols = getBinFeatures(data)
    result  = [x for x in numeric_columns if x not in bin_cols] 
    return result

def getContinuousColumns(data): 
    threshold = 10
    numeric_columns = data.select_dtypes(include=['int16', 'float', 'int32', 'int64']).columns.tolist()
    cont_cols = data[numeric_columns].columns[(data[numeric_columns].nunique() ) > threshold]
   
    return cont_cols

def getNumericDescreteColumns(data): 
    threshold = 10
    numeric_columns = data.select_dtypes(include=['int16', 'float', 'int32', 'int64']).columns.tolist()
    cont_cols = data[numeric_columns].columns[(data[numeric_columns].nunique() ) <= threshold]
   
    return cont_cols

def doValueImputationForVariance(data):
    data1 = meanImputationforNumericalFeatures(data)
    data2 = fillMissingNominalNulls(data)
    data3 = fillMissingOrdinalNulls(data)
    combined_df = pd.concat([data1, data2, data3], axis = 1)
    return combined_df

def getZeroVarianceColumns(data, doImputation): 
    if doImputation == True:
        df = doValueImputationForVariance(data)
        same_value_columns = df.columns[df.nunique() == 1].tolist()
        
    else:
        same_value_columns = data.columns[data.nunique() == 1].tolist()
        
    return same_value_columns

def getBinFeatures(data):
    binary_columns = data.select_dtypes(include = ['bool','object', 'int']).columns
    bool_columns = binary_columns[data[binary_columns].apply(lambda x: x.dropna().isin([True, False]).all())]
    bin_cols = data.columns[data.apply(lambda x: set(x.dropna()).issubset({0,1}))]
    bin_cols = bin_cols.tolist()
    bool_columns = bool_columns.tolist()
    return list(set(bool_columns+bin_cols))

def transformcoltobool(data,col):
    new_column_names = ['missing_original'+ col]
    filtered_df= data[col].copy()
    filtered_df.columns = new_column_names
    filtered_df.fillna('Novalue', inplace = True)
    filtered_df = filtered_df.replace({False:'hasvalue', True: 'hasvalue', 1:'hasvalue', 0:'hasvalue'})
    combined_df = pd.concat([data, filtered_df], axis = 1)
    return combined_df

def generateColumnsForMissingBool(data):
    bincols = getBinFeatures(data)
    null_cols = data[bincols].columns[data[bincols].isnull().any()]
    null_cols = list(null_cols)
    filtered_df = data[null_cols].copy()
    new_column_names = ['missing_original'+ column for column in filtered_df.columns]
    filtered_df.columns = new_column_names
    filtered_df.fillna('Novalue', inplace = True)
    filtered_df = filtered_df.replace({False:'hasvalue', True: 'hasvalue', 1:'hasvalue', 0:'hasvalue'})
    combined_df = pd.concat([data, filtered_df], axis = 1)
    return combined_df

def modeImputationforBoolFeatures(data):
    bool_cols = getBinFeatures(data)
    mode = data[bool_cols].mode().iloc[0]
    data[bool_cols] = data[bool_cols].fillna(mode)
    return data

def getNominalFeatures(data):
    date_feat = getDateFeatures(data)
    nominal_df = data.select_dtypes(include =['object', 'category'])
    bin_cols = getBinFeatures(data)
    ord_features = getOrdinalFeatures()
    result1 = nominal_df[[col for col in nominal_df.columns if col not in date_feat]]
    result1 = result1[[col for col in result1.columns if col not in bin_cols]]
    result1 =  result1[[col for col in result1.columns if col not in ord_features]]
    return list(result1.columns)

def fillMissingNominalNulls(data):
    nominal_cols = getNominalFeatures(data)
    data[nominal_cols] = data[nominal_cols].fillna('Novalue')
    return data

def fillMissingOrdinalNulls(data):
    ordinal_cols = getOrdinalFeatures()
    data[ordinal_cols] = data[ordinal_cols].fillna('Novalue')
    return data

def fillDateNulls(data):
    date_feat = getDateFeatures(data)
    for column in date_feat:
        data[column] = data[column].apply(convertDateTime)
    return data

def convertDateTime(datetime_str):
    if pd.isnull(datetime_str):
        return '1900-01-01T00:00:00.000Z'
    else:
        datetime_obj = pd.to_datetime(datetime_str, format='%Y-%m-%dT%H:%M:%S.%fZ')
        return datetime_obj.strftime('%Y-%m-%dT%H:%M:%S.000Z')

def convertDateToNumeric(data, date_columns):
    date_format = '%Y-%m-%dT%H:%M:%S.000Z'
    for column in date_columns:
        conv = pd.to_datetime(data[column], format=date_format, errors='coerce')
        excel_dates = [(date - datetime(1899, 12 ,30)).total_seconds()/ (24*60*60) + 1 for date in conv]
        data[column] = excel_dates
    return data    

def getNumericalColumns(data):
    numeric_columns = data.select_dtypes(include=['int16', 'float', 'int32', 'int64']).columns.tolist()
    return numeric_columns

def fillMissingNumericNulls(data):
    nominal_cols = getNumericalColumns(data)
    data[nominal_cols] = data[nominal_cols].fillna('novalue')
    return data

def generateColumnsForMissingNumerical(data):
    num_cols = getNumericalColumns(data)
    null_cols = data[num_cols].columns[data[num_cols].isnull().any()]

    filtered_df = data[null_cols].copy()
    new_column_names = ['missing_original'+ column for column in filtered_df.columns]
    filtered_df.columns = new_column_names
    #replacing with the 0 and 1
    filtered_df.fillna('Novalue', inplace = True)
   
    filtered_df = filtered_df.applymap(lambda x:'hasValue' if x != 'Novalue' else x)
    combined_df = pd.concat([data, filtered_df], axis = 1)
    return combined_df

def meanImputationforNumericalFeatures(data):
    numeric_cols = getNumericalColumns(data)
    mean = data[numeric_cols].mean()
    data[numeric_cols] = data[numeric_cols].fillna(mean)
    return data
    
def getDiscreteFeatures(data):
    nf = getNominalFeatures(data)
    of = getOrdinalFeatures()
    cat_columns = nf + of
    threshold = 5 
    discrete_categorical_columns = data[cat_columns].columns[(data[cat_columns].nunique() ) <= threshold]
    return discrete_categorical_columns

def generateColumnsForMissingContinuous(data):
    cont_cols = getContinuousColumns(data)
    null_cols = data[cont_cols].columns[data[cont_cols].isnull().any()]

    filtered_df = data[null_cols].copy()
    new_column_names = ['missing_original'+ column for column in filtered_df.columns]
    filtered_df.columns = new_column_names
    #replacing with the 0 and 1
    filtered_df.fillna('Novalue', inplace = True)
   
    filtered_df = filtered_df.applymap(lambda x:'hasvalue' if x != 'Novalue' else x)
    combined_df = pd.concat([data, filtered_df], axis = 1)
    return combined_df

def medianImputationforContinuousFeatures(data): 
    cont_cols = getContinuousColumns(data)
    median = data[cont_cols].median()
    data[cont_cols] = data[cont_cols].fillna(median)
    return data

def scaleNumericColumns(data):
    numeric_columns = data.select_dtypes(include=['int16', 'float', 'int32', 'int64']).columns.tolist()
    exclude_columns = ['subjectIdentifier.version', 'reportableData.tradeParty1TransactionId',             'subjectIdentifier.transactionId']
    numeric_columns = [col for col in numeric_columns if col not in exclude_columns]
    selected_num_cols = [col for col in numeric_columns if data[col].max() > 1]
    if selected_num_cols:
        scaler = MinMaxScaler() #Argument
        data[selected_num_cols] = scaler.fit_transform(data[selected_num_cols])
        data[selected_num_cols] = round(data[selected_num_cols], 2) 
    return data

def calculateBins(data, bin_size):
    # Step 3: Calculate the number of bins using Scott's Normal Reference Rule
    data_range = data.max() - data.min()
    num_bins = int(np.ceil(data_range / bin_size))
    return max(num_bins, 2)

def applyEqualWithBinning(data, column, num_bins):
    column_data = data[column].dropna()
    unique_values = column_data.unique()
    if len(unique_values) > 1 and num_bins > 1:
        bin_edges = np.linspace(column_data.min(), column_data.max(), num_bins + 1)
        binned_data = np.digitize(column_data, bin_edges) - 1
        return pd.cut(column_data, bins=bin_edges, labels=generateBinLabels(bin_edges), include_lowest=True)
    return None

def generateBinLabels(bin_edges):
    bin_edges = np.unique(bin_edges)
    bin_labels = [f'Between {bin_edges[i]:.2f} and {bin_edges[i+1]:.2f}' for i in range(len(bin_edges)     - 1)]
    return bin_labels

def applyBinninginAllNumCol(data):
    bin_size = 0.10
    numeric_columns = data.select_dtypes(include=['int16', 'float', 'int32', 'int64']).columns.tolist()
    for col in numeric_columns:
        if col not in ['subjectIdentifier.version', 'reportableData.tradeParty1TransactionId',             'subjectIdentifier.transactionId']:
            num_bins = calculateBins(data[col], bin_size)
            bin_labels = applyEqualWithBinning(data, col, num_bins)
            if bin_labels is not None:
                data[col] = bin_labels        
    return data

def transformColValues(data):
        data['nonReportableData.party2Ciscode'] = data['nonReportableData.party2Ciscode'].astype(str).apply(lambda x: f'cisCode_{len(x)}' if x.strip() != 'Missing' else 'cisCode_0').astype(str)
        data['reportableData.utiId'] = data['reportableData.utiId'].astype(str).apply(lambda x: f'uti_{len(x)}' if x.strip() != 'Missing' else 'uti_0').astype(str)
        data['reportableData.utiIdPrefix'] = data['reportableData.utiIdPrefix'].astype(str).apply(lambda x: f'utiPrefix_{len(x)}' if x.strip() != 'Missing' else 'utiPrefix_0').astype(str)
        #data['reportableData.complexTradeComponentId'] = data['reportableData.complexTradeComponentId'].astype(str).apply(lambda x: f'ComponentId_{len(x)}' if x.strip() != 'Missing' else 'ComponentId_0').astype(str)
        data['reportableData.tradeParty2CountryOfOtherCpty'] = data['reportableData.tradeParty2CountryOfOtherCpty'].astype(str).apply(lambda x: f'tp2Country_{len(x)}' if x.strip() != 'Missing' else 'tp2Country_0').astype(str)
        data['reportableData.tradeParty2Id'] = data['reportableData.tradeParty2Id'].astype(str).apply(lambda x: f'tradeParty2Id_{len(x)}' if x.strip() != 'Missing' else 'tradeParty2Id_0').astype(str)
        return data

def dropColumns(data):
    data = data.drop(columns=['subjectIdentifier.version','nonReportableData.tntrReceivedTimestamp','transactionReportingStatus.stateTransitionEffectiveDateTime','reportableData.reportingTimestamp','transactionReportingStatus.stateTransitionDateTime','nonReportableData.tradeParty2Name','reportableData.complexTradeComponentId', 'nonReportableData.tradeVersion', 'reportableData.messageId'])
    
    return data

def dropColumn(data,col):
    data = data.drop(columns=[col])
    return data

df= pd.read_csv('TNTR_trade_prodprl_GFX.csv')

data=df
data = generateColumnsForMissingBool(data)
data = modeImputationforBoolFeatures(data)
data = fillMissingNominalNulls(data)
data = fillMissingOrdinalNulls(data)
data = fillDateNulls(data)
data = fillMissingNumericNulls(data)
data = data.drop(getZeroVarianceColumns(data, False), axis=1)

data=removeAbsoluteLinear(data)

df=data

df=df.drop(columns=['nonReportableData.isLastAcked'])
df=df.drop(columns=['nonReportableData.isLatestVersion'])

df['nonReportableData.party2NucId'] = df['nonReportableData.party2NucId'].fillna('Novalue')
df['nonReportableData.party2NucId'] = df['nonReportableData.party2NucId'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df['nonReportableData.creditSupportID'] = df['nonReportableData.creditSupportID'].fillna('Novalue')
df['nonReportableData.creditSupportID'] = df['nonReportableData.creditSupportID'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df['nonReportableData.bookId'] = df['nonReportableData.bookId'].fillna('Novalue')
df['nonReportableData.bookId'] = df['nonReportableData.bookId'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df['nonReportableData.party1BrokerNucId'] = df['nonReportableData.party1BrokerNucId'].fillna('Novalue')
df['nonReportableData.party1BrokerNucId'] = df['nonReportableData.party1BrokerNucId'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df=df.drop(columns=['nonReportableData.tradeVersion'])

df = scaleNumericColumns(df)

df=df.drop(columns=['reportableData.messageId'])

df=df.drop(columns=['reportableData.tradeParty1TransactionId'])

df['reportableData.complexTradeComponentId'] = df['reportableData.complexTradeComponentId'].fillna('Novalue')
df['reportableData.complexTradeComponentId'] = df['reportableData.complexTradeComponentId'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

cont_feat=getNominalFeatures(df)
cols=getZeroVarianceColumns(df[cont_feat],False)

df = df.drop(getZeroVarianceColumns(df[cont_feat], False), axis=1)

df['nonReportableData.tradeParty2Name'] = df['nonReportableData.tradeParty2Name'].fillna('Novalue')
df['nonReportableData.tradeParty2Name'] = df['nonReportableData.tradeParty2Name'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df['nonReportableData.party2Ciscode'] = df['nonReportableData.party2Ciscode'].fillna('Novalue')
df['nonReportableData.party2Ciscode'] = df['nonReportableData.party2Ciscode'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df['nonReportableData.party1Ciscode'] = df['nonReportableData.party1Ciscode'].fillna('Novalue')
df['nonReportableData.party1Ciscode'] = df['nonReportableData.party1Ciscode'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df['reportableData.utiIdPrefix'] = df['reportableData.utiIdPrefix'].fillna('Novalue')
df['reportableData.utiIdPrefix'] = df['reportableData.utiIdPrefix'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df['reportableData.tradeParty2Id'] = df['reportableData.tradeParty2Id'].fillna('Novalue')
df['reportableData.tradeParty2Id'] = df['reportableData.tradeParty2Id'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

df=df.drop(columns=['missing_originalnonReportableData.isLastAcked'])

df['reportableData.productClassification'] = df['reportableData.productClassification'].fillna('Novalue')
df['reportableData.productClassification'] = df['reportableData.productClassification'].apply(lambda x: 'hasvalue' if x != 'Novalue' else x)

threshold = len(df) * 0.15

countryCpty_counts = df['reportableData.tradeParty2CountryOfOtherCpty'].value_counts()
less_frequent_CountryCpty = countryCpty_counts[countryCpty_counts < threshold].index
def group_countryCpty(countryCpty):
    return countryCpty if countryCpty not in less_frequent_CountryCpty else 'Other'
df['reportableData.tradeParty2CountryOfOtherCpty'] = df['reportableData.tradeParty2CountryOfOtherCpty'].apply(group_countryCpty)

notional_currency_counts = df['reportableData.leg1NotionalCurrency'].value_counts()
less_frequent_notional = notional_currency_counts[notional_currency_counts < threshold].index
def group_currency_notional(currency):
    return currency if currency not in less_frequent_notional else 'Other'
df['reportableData.leg1NotionalCurrency'] = df['reportableData.leg1NotionalCurrency'].apply(group_currency_notional)

notional2_currency_counts = df['reportableData.leg2NotionalCurrency'].value_counts()
less_frequent_notional2 = notional2_currency_counts[notional2_currency_counts < threshold].index
def group_currency_notional2(currency):
    return currency if currency not in less_frequent_notional2 else 'Other'
df['reportableData.leg2NotionalCurrency'] = df['reportableData.leg2NotionalCurrency'].apply(group_currency_notional2)

leg1_currency_counts = df['reportableData.leg1ExchangeRateBasisCurrency'].value_counts()
less_frequent_leg1 = leg1_currency_counts[leg1_currency_counts < threshold].index
def group_currency_leg1(currency):
    return currency if currency not in less_frequent_leg1 else 'Other'
df['reportableData.leg1ExchangeRateBasisCurrency'] = df['reportableData.leg1ExchangeRateBasisCurrency'].apply(group_currency_leg1)

leg2_currency_counts = df['reportableData.leg2ExchangeRateBasisCurrency'].value_counts()
less_frequent_leg2 = leg2_currency_counts[leg2_currency_counts < threshold].index
def group_currency_leg2(currency):
    return currency if currency not in less_frequent_leg2 else 'Other'
df['reportableData.leg2ExchangeRateBasisCurrency'] = df['reportableData.leg2ExchangeRateBasisCurrency'].apply(group_currency_leg2)

df=dropColumn(df,'nonReportableData.tntrReceivedTimestamp')
df=dropColumn(df,'reportableData.reportingTimestamp')
df=dropColumn(df,'transactionReportingStatus.stateTransitionDateTime')
df=dropColumn(df,'transactionReportingStatus.stateTransitionEffectiveDateTime')

df=transformcoltobool(df,'nonReportableData.eventExecutionTimestamp')
df=dropColumn(df,'nonReportableData.eventExecutionTimestamp')

df=transformcoltobool(df,'reportableData.originalExecutionTimestamp')
df=dropColumn(df,'reportableData.originalExecutionTimestamp')

df=transformcoltobool(df,'reportableData.terminationDate')
df=dropColumn(df,'reportableData.terminationDate')

#binning
date_feat = getDateFeatures(df)
df = convertDateToNumeric(df, date_feat)
df = scaleNumericColumns(df)

df=df.drop(columns=['reportableData.utiId'])

threshold = len(df) * 0.1

df['nonReportableData.maturityDate'] = df['nonReportableData.maturityDate'].fillna('Novalue')
df['nonReportableData.maturityDate'] = df['nonReportableData.maturityDate'].apply(lambda x: 'hasvalue' if x != 'novalue' else x)

df['nonReportableData.startDate'] = df['nonReportableData.startDate'].fillna('novalue')
df['nonReportableData.startDate'] = df['nonReportableData.startDate'].apply(lambda x: 'hasvalue' if x != 'novalue' else x)

threshold = len(df) * 0.1
leg2_currency_counts = df['reportableData.exchangeRate'].value_counts()
less_frequent_leg2 = leg2_currency_counts[leg2_currency_counts < threshold].index
def group_currency_leg2(currency):
    return currency if currency not in less_frequent_leg2 else 'Other'
df['reportableData.exchangeRate'] = df['reportableData.exchangeRate'].apply(group_currency_leg2)


threshold = len(df) * 0.1
leg2_currency_counts = df['reportableData.leg1NotionalAmount'].value_counts()
less_frequent_leg2 = leg2_currency_counts[leg2_currency_counts < threshold].index
def group_currency_leg2(currency):
    return currency if currency not in less_frequent_leg2 else 'Other'
df['reportableData.leg1NotionalAmount'] = df['reportableData.leg1NotionalAmount'].apply(group_currency_leg2)

threshold = len(df) * 0.01
leg2_currency_counts = df['reportableData.leg2NotionalAmount'].value_counts()
less_frequent_leg2 = leg2_currency_counts[leg2_currency_counts < threshold].index
def group_currency_leg2(currency):
    return currency if currency not in less_frequent_leg2 else 'Other'
df['reportableData.leg2NotionalAmount'] = df['reportableData.leg2NotionalAmount'].apply(group_currency_leg2)

threshold = len(df) * 0.07

df= df.drop(columns='subjectIdentifier.version')

df = df.drop(getZeroVarianceColumns(df, False), axis=1)

dfid=pd.read_csv('TNTR_trade_prodprl_GFX.csv')

combined_df = pd.concat([df, dfid['subjectIdentifier.version'], dfid['subjectIdentifier.transactionId']], axis = 1)
combined_df.to_csv(r'stage1_normalized.csv', index = False)
